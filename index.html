<!DOCTYPE html>
<html>
	<head>
		<link href="css/bootstrap.min.css" rel="stylesheet" media="screen">
		<script src="http://code.jquery.com/jquery-latest.js"></script>
		<script src="js/bootstrap.min.js"></script>
		<style type="text/css">
			.illustration {
				float: right;
			}
		</style>
		<title>Go Database Architecture!</title>
	</head>
	<body data-spy="scroll" data-target="#sidebar">
		<div class="row">
			<div id="sidebar" class="span3">
				<ul class="nav nav-list affix">
					<li><a href="#overview">Overview</a></li>
					<li><a href="#namespace">Namespace</a></li>
					<li><a href="#structure">Structure</a></li>
					<li><a href="#setops">Set operations</a></li>
					<li><a href="#persistence">Persistence</a></li>
					<li><a href="#routing">Routing</a></li>
					<li><a href="#responsibility">Responsibility</a></li>
					<li><a href="#redundancy">Redundancy</a></li>
					<li><a href="#timestamps">Timestamps</a></li>
					<li><a href="#synchronization">Synchronization</a></li>
					<li><a href="#cleaning">Cleaning</a></li>
					<li><a href="#migration">Migration</a></li>
				</ul>
			</div>
			<div class="span9">
				<h1>god</h1>
				<h2 id="overview">Overview</h2>
				<p>
				god is a scalable, performant, persistent in memory data structure server. It allows massively distributed applications to update and fetch common data in a structured and sorted format.
				</p>
				<p>
				Its main inspirations are <a href="http://redis.io/">Redis</a> and <a href="http://en.wikipedia.org/wiki/Chord_(peer-to-peer)">Chord</a>/<a href="http://pdos.csail.mit.edu/papers/dhash:nsdi/paper.pdf">DHash</a>.
				Like <a href="http://redis.io/">Redis</a> it focuses on performance, ease of use and a small, simple yet powerful feature set, 
				while from the <a href="http://en.wikipedia.org/wiki/Chord_(peer-to-peer)">Chord</a>/<a href="http://pdos.csail.mit.edu/papers/dhash:nsdi/paper.pdf">DHash</a> projects it inherits scalability, redundancy and transparent failover behaviour.
				<p>
				To try it out right now, <code>go get github.com/zond/god/server</code>, then run <code>server</code> and browse to <a href="http://localhost:9192/">http://localhost:9192/</a>.
				</p>
				<p>
				For API documentation, go to <a href="http://go.pkgdoc.org/github.com/zond/god">http://go.pkgdoc.org/github.com/zond/god</a>.
				</p>
				<p>
				For the source, go to <a href="https://github.com/zond/god">https://github.com/zond/god</a>.
				</p>
				<h2 id="namespace">Namespace</h2>
				<h3>Chord circle</h3>
				<object class="illustration" data="chord-circle.svg" type="image/svg+xml"></object>
				<p>
				The namespace of god, as for the <a href="http://en.wikipedia.org/wiki/Chord_(peer-to-peer)">Chord</a> project, is a conceptual circle where each actor, be it database node or data item, has a position. Usually the circle overflows at the maximum value for a selected hash function used to spread keys over the namespace.
				</p>
				<h3>Murmur</h3>
				<p>
				The hash function used in god is <a href="http://code.google.com/p/smhasher/wiki/MurmurHash3">MurmurHash3</a>, a fairly new and extremely fast hash function. Unlike many other hash functions it makes no cryptographical claims, but focuses solely on high speed and low collision distribution.
				</p>
				<p>
				god contains a slightly modified (only to let it compile in a C compiler) of the 128 bit, x64 version of <a href="http://code.google.com/p/smhasher/wiki/MurmurHash3">MurmurHash3</a> wrapped in a thin layer of Go.
				</p>
				<h3>Hashed vs non hashed keys</h3>
				<p>
				The main reason to hash keys before inserting them into the namespace is to avoid concentrating keys in a part of the circle, to facilitate letting all nodes share as equal an amount of data as possible.
				</p>
				<p>
				However, since it could be very useful for users of a database to store ordered data, or to wilfully concentrate certain data on certain parts of the cluster, god does not force the user to hash the keys.
				</p>
				<p>
				Instead, the nodes in god are able to migrate along the circle to hand off data to one another when an imbalance is detected.
				</p>
				<p>
				This would allow, for example, data for a geographical region to be prefixed with a code, and making data with that prefix go on designated servers placed in the same geographical region. The cluster could also be configured so that all backups for data positioned in one region was positioned in other regions for greated safety in the face of disasters of different kinds.
				</p>
				<p>
				Currently there is no option to either limit node migrations (to force nodes in one datacenter to stay in one part of the namespace), or to force backups to live on nodes in other datacenters, but it would be fairly easy, conceptually, to add in a future version if the need arose.
				</p>
				<h2 id="structure">Structure</h2>
				<h3>Radix trees</h3>
				<object class="illustration" data="radix-tree.svg" type="image/svg+xml"></object>
				<p>
				To map keys to values, a mapping structure is needed. For infrastructural reasons (<a href="#synchronization">synchronization</a> and <a href="#cleaning">cleaning</a>) as well as for functionality of different kinds, we need a sorted mapping, and it has to be deterministically structured.
				</p>
				<p>
				Radix trees have all these attributes, and are somewhat frugal when it comes to memory consumption due to the common parts of keys only being stored once.
				</p>
				<p>
				In the illustration to the right, nodes only contain the part of their key differing from the key of their parent, and the nodes marked red don't contain data at all but only contain common parts of keys for their children. Note that the root node contains the empty key, and in this case it actually has a value as well.
				</p>
				<h3>Byte values</h3>
				<p>
				All nodes in the main tree can contain byte values, which can be encoded to contain strings, integers, floats or whatever kind of data one wants to store.
				</p>
				<h3>Sub trees</h3>
				<object class="illustration" data="tree-values.svg" type="image/svg+xml"></object>
				<p>
				In addition, since the need for structured data (in the sense that one can sort, paginate, slice and dice it) is one of the things that makes a vanilla key/value store hard to use in many common use cases, each key in a Tree can also contain a separate sub tree.
				</p>
				<p>
				This allows us to, for example, store user metadata in the top level byte values, while storing the set of friends in a sub tree. Then we can easily fetch, slice and store friends without having to reserialize the user data (or even worse, the entire set of friends).
				</p>
				<h3>Mirror trees</h3>
				<object class="illustration" data="mirror-trees.svg" type="image/svg+xml"></object>
				<p>
				To allow us to sort, paginate, slice and dice the data depending on not only key, but also value, each tree can also be 'mirrored'. This, in essence, means that the tree will contain another tree that is exactly mirroring the state of the first tree, except that the keys of the outer tree is the values of the inner tree and vice versa.
				<p>
				<p>
				To avoid colliding keys in the mirror trees, when several keys in the outer tree have the same values, each key in the mirror tree is appended with the corresponding value. Those appendages are removed, however, when key/value pairs are fetched or sliced from the mirror tree.
				</p>
				<h3>Configuration</h3>
				<p>
				To control how trees behave, for example (and currently only) to control whether they are mirrored or not, there is a configuration map for each sub tree. Setting the key <code>mirrored</code> to <code>yes</code> will make the tree keep a mirror.
				</p>
				<h2 id="setops">Set operations</h2>
				<object class="illustration" data="setops.svg" type="image/svg+xml"></object>
				<h3>Operations</h3>
				<p>
				The sub trees in god can be used for various kinds of set operations: union, intersection, subtraction and xor.
				</p>
				<p>
				Since all sets in god are both sorted and possible to skip through at logarithmic time, the set operations are usually efficient and take time depending on the logarithm of the size of the source sets.
				</p>
				<p>
				god includes a simple s-expression parser that parses these kinds of expressions. The code <code>(I (U set1 set2 set3) (U set4 set5 set6))</code>, for example, will calculate the intersection of the unions of set1-3 and set4-6.
				</p>
				<h3>Mergers</h3>
				<p>
				Since many set operations (such as intersections and unions) may produce values from a combination of source sets, there is a concept of merge function. Merge functions take a list of lists of input values, and produce a list of output values.
				</p>
				<p>
				These merge operations range from trivial, like ConCat that simply concatenates the bytes in all input values and puts the result in a single element list and Append that simply creates an output list out of all values in the input lists.
				</p>
				<p>
				To use a merger other than Append (which is the default), simply add <code>:MERGERNAME</code> to the function symbol in your s-expression. For example: <code>(I:ConCat set1 (U:ConCat set2 set3))</code> will concatenate the union of set2-3, then concatenate those results to the intersection with set1.
				</p>
				<h3>Encodings</h3>
				<p>
        To enable the merge operations to do more interesting things, and to simplify for users of god to store more interesting things than byte slices in the database, there are predefined encodings implemented for encoding integers, floats and bigints into byte slices.
				</p>
				<p>
				If such an encoding is used, it is also possible to use more clever mergers, such as IntegerSum, FloatMul or BigIntAnd.
				</p>
				<h3>Parameters</h3>
				<p>
				Set expressions can be given start and stop conditions, such as the minimum key to start with, the maximum key to look at, or the maximum number of items to return. This way even the set expressions can be paginated and treated as slices.
				</p>
				<p>
				If a set expression also has a destination provided, the results of the set expression will not be returned to the caller, but put inside the sub tree defined by the destination key.
				</p>
				<h2 id="persistence">Persistence</h2>
				<object class="illustration" data="logfiles.svg" type="image/svg+xml"></object>
        <h3>Log files</h3>
				<p>
				god nodes stay persistent between restarts by always <a href="#synchronization">synchronizing</a> with sibling nodes. But when the entire cluster needs to be restarted, some kind of disk based persistence is needed.
				</p>
				<p>
				For that purpose a log file based persistence engine logs all mutating operations (put, delete, clear, configure) performed by each node and streams them to disk.
				</p>
				<h3>Snapshots</h3>
				<p>
				Since the same keys will often be changed many times over during the lifetime of a database, only log files would quickly prove inefficient, since the same log file would contain multiple puts, deletes and new puts of the same key.
				</p>
				<p>
				To alleviate this, snapshot files can be created that contain the merged sum of all operations in a set of log files. Only the latest operation on each key in the snapshot is recorded to get rid of redundant operations.
				</p>
				<p>
				god does this by reading the latest snapshot and all log files following it into a set of nested maps, overwriting and deleting the map data as new operations stream in from the log files. Then the map set is streamed into a snapshot file.</p>
				<h3>Automatic snapshotting</h3>
				<p>
				To avoid cluttering the disk with huge log files containing redundant data, god always automatically creates new snapshots when the last log file grows too largs. This is done in a separate goroutine and thus does not have to take away from the performance of the node, but it will require extra memory to build the map set.
				</p>
				<h2 id="routing">Routing</h2>
				<object class="illustration" data="ping_notify.svg" type="image/svg+xml"></object>
				<h3>Notify</h3>
				<p>
				Each node in the cluster regularly notifies its successor of its presence. The successor, in its turn, will add the notifier to its routing table and respond with the node it considers to be its predecessor.
				</p>
				<p>
				The notifier will compare the returned predecessor to itself, and add it to its own routing table if different.
				</p>
				<h3>Ping</h3>
				<p>
				Each node also takes responsibility for regularly pinging its successor with a packet containing itself and a hash of its routing table.
				</p>
				<p>
				The predecessor will add the successor to its routing table and compare the hash with that of its own routing table. If different, it will also request the complete routing table from the successor.
				</p>
				<p>
				The predecessor will then clean its routing table by removing all nodes between its confirmed (being a notifier) predecessor and itself, so that nodes having been removed after a failed ping will not get reintroduced by a successor.
				</p>
				<h2 id="responsibility">Responsibility</h2>
				<h3>Successors</h3>
				<p>
				Like in <a href="http://en.wikipedia.org/wiki/Chord_(peer-to-peer)">Chord</a>/<a href="http://pdos.csail.mit.edu/papers/dhash:nsdi/paper.pdf">DHash</a>, each node is responsible for all items between its predecessor and itself. Since the predecessor is the one node each node keeps pinging, this works out well since we can trust each node to know if a key is between its predecessor and itself.
				</p>
				<h2 id="redundancy">Redundancy</h2>
				<object class="illustration" data="backups.svg" type="image/svg+xml"></object>
				<h3>Notify</h3>
				<h3>N successive backups</h3>
				<p>
				To allow for a transparent failover, where the new owner of a set of data already has it available, god lets each node keep a backup of the data of a number of its predecessors.
				</p>
				<p>
				If a node disappears, this will allow the cluster to stay in a valid state, since the new owner already has the backup (now master) copy of the data in its local storage.
				</p>
				<h2 id="timestamps">Timestamps</h2>
				<h3>Latest write wins</h3>
				<p>
				Since god is intended to work with hundreds or even thousands of nodes, it is likely that some nodes will experience temporary outages or net splits. Since god does not have eventual consistency and versioned entries, a situation where the cluster is split in isolated parts still accessed by clients will lead to the rejoined cluster containing a mix of the latest writes for all rejoined parts.
				</p>
				<p>
				However, as long as the disconnected parts of the cluster are not used for writing (being crashed, completely disconnected or just turned off), timestamps do a sufficient job of avoiding conflicts.
				</p>
				<h3>Undone changes</h3>
				<p>
				When a node is somehow disconnected and reconnected without being emptied of all data, it will potentially contain stale values for all its contained keys.
				</p>
				<p>
				To avoid this stale data being redistributed across the cluster, each value has a timestamp and newer timestamps always override older timestamps.
				</p>
				<h3>Zombies</h3>
				<object class="illustration" data="zombies.svg" type="image/svg+xml"></object>
				<p>
				If a node gets disconnected and reconnected without being cleared in between, it could conceivably contain data that was removed in its absence.
				</p>
				<p>
				To avoid these values being reanimated and haunt you as zombie data, each delete or clear (except the <code>kill</code> command, ironically) will result in tombstones replacing the data you remove. These tombstones are timestamped just like any other piece of data, and will disperse through the cluster just like a put.
				</p>
				<h3>Time network</h3>
				<p>
				To make sure that the timestamps as far as possible reflect the actual sequence of events in the cluster, and avoid putting additional burdens on the operations part of things, god contains its own time network.
				</p>
				<p>
				The time network in god is an extremely simple one, though, and makes no claim to knowing the real time at any point. It simply synchronizes clocks randomly across the cluster, and dilates time for each server so that they converge on as small a difference as possible.
				</p>
				<h2 id="synchronization">Synchronization</h2>
				<h3>Merkle trees</h3>
				<h3>Comparing hashes</h3>
				<h2 id="cleaning">Cleaning</h2>
				<h3>Destructive synchronization</h3>
				<h2 id="migration">Migration</h2>
				<h3>Imbalance</h3>
				<h3>Changing positions</h3>
			</div>
		</div>
	</body>
</html>
